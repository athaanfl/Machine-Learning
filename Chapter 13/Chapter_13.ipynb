{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13: Loading and Preprocessing Data with TensorFlow - Notebook Reproduksi Kode\n",
        "\n",
        "Bab ini membahas cara efisien memuat dan memproses dataset besar menggunakan\n",
        "TensorFlow's `tf.data` API, yang sangat penting untuk membangun pipeline data\n",
        "yang skalabel dan efisien untuk Deep Learning.\n",
        "\n",
        "Kita akan melihat:\n",
        "- tf.data API untuk membuat pipeline input.\n",
        "- Membangun pipeline sederhana (from_tensor_slices, batch, shuffle, prefetch).\n",
        "- Transformasi data (map, filter, interleave).\n",
        "- Menguraikan data TensorFlow Record.\n",
        "- tf.io.TFRecordWriter dan TFRecordDataset.\n",
        "- Input preprocessor menggunakan Keras Preprocessing Layers."
      ],
      "metadata": {
        "id": "fIPGIQseDajM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHdI4faSDXDh",
        "outputId": "3414cb4f-f364-4da9-ef8b-bf2a4f8eccb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- tf.data API: Membangun Pipeline Sederhana ---\n",
            "(<tf.Tensor: shape=(8,), dtype=float64, numpy=\n",
            "array([-0.19397883, -1.07781319, -0.94338545,  0.01485314,  0.02073335,\n",
            "       -0.57291624,  0.92926047, -1.42215523])>, <tf.Tensor: shape=(), dtype=float64, numpy=1.442>)\n",
            "(<tf.Tensor: shape=(8,), dtype=float64, numpy=\n",
            "array([ 0.75198318, -1.868895  ,  0.40547793, -0.23327682,  1.8614649 ,\n",
            "        0.20516532, -0.91654738,  1.09666969])>, <tf.Tensor: shape=(), dtype=float64, numpy=1.687>)\n",
            "(<tf.Tensor: shape=(8,), dtype=float64, numpy=\n",
            "array([-0.41469108,  0.02970134,  0.81808819,  1.05678372, -0.08786707,\n",
            "       -0.29983271,  1.30872858, -1.697027  ])>, <tf.Tensor: shape=(), dtype=float64, numpy=1.621>)\n",
            "\n",
            "Melatih model dengan tf.data.Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model berhasil dilatih dengan tf.data.Dataset.\n",
            "Loss terakhir: 0.5577\n",
            "\n",
            "--- tf.data API: Chain Transformasi ---\n",
            "Bentuk X_batch setelah map: (32, 13)\n",
            "Bentuk y_batch setelah map: (32,)\n",
            "Jumlah batch setelah filter (y > 2.0 per item): 155\n",
            "\n",
            "--- tf.data API: Interleave ---\n",
            "Dibuat 5 file CSV dummy di ./dummy_data\n",
            "\n",
            "Interleaved data (beberapa baris):\n",
            "0.3745401188473625,0.9507143064099162,0.7319939418114051\n",
            "0.0516817211686077,0.531354631568148,0.5406351216101065\n",
            "0.5986584841970366,0.15601864044243652,0.15599452033620265\n",
            "0.6374299014982066,0.7260913337226615,0.9758520794625346\n",
            "0.05808361216819946,0.8661761457749352,0.6011150117432088\n",
            "\n",
            "--- Menguraikan Data TensorFlow Record ---\n",
            "Data berhasil ditulis ke TFRecord.\n",
            "\n",
            "Membaca dan menguraikan TFRecord (beberapa contoh):\n",
            "Bentuk fitur: (3, 8)\n",
            "Bentuk label: (3, 1)\n",
            "Contoh fitur (pertama): [-0.19397883 -1.0778131  -0.9433854   0.01485314  0.02073335 -0.57291627\n",
            "  0.9292605  -1.4221553 ]\n",
            "Contoh label (pertama): [1.442]\n",
            "\n",
            "--- Keras Preprocessing Layers ---\n",
            "Mengadaptasi Normalization layer...\n",
            "Normalization layer berhasil diadaptasi.\n",
            "\n",
            "Melatih model dengan Normalization layer...\n",
            "Model dengan Normalization layer berhasil dilatih.\n",
            "Loss terakhir: 0.4035\n",
            "\n",
            "--- Selesai Reproduksi Kode Chapter 13 ---\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Chapter 13: Loading and Preprocessing Data with TensorFlow - Reproduksi Kode Lengkap\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Setup\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"tf_data\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- 1. tf.data API ---\n",
        "\n",
        "print(\"--- tf.data API: Membangun Pipeline Sederhana ---\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train))\n",
        "for item in dataset.take(3):\n",
        "    print(item)\n",
        "\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 100\n",
        "n_epochs = 5\n",
        "\n",
        "dataset = dataset.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(1)\n",
        "\n",
        "print(\"\\nMelatih model dengan tf.data.Dataset...\")\n",
        "model_tf_data = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train_scaled.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model_tf_data.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=0.001))\n",
        "history_tf_data = model_tf_data.fit(dataset, epochs=n_epochs, verbose=0)\n",
        "print(\"Model berhasil dilatih dengan tf.data.Dataset.\")\n",
        "print(f\"Loss terakhir: {history_tf_data.history['loss'][-1]:.4f}\")\n",
        "\n",
        "# --- Transformasi Chain (map, prefetch) ---\n",
        "print(\"\\n--- tf.data API: Chain Transformasi ---\")\n",
        "dataset_complex = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train)).shuffle(100).batch(batch_size)\n",
        "\n",
        "def add_extra_features(X_batch, y_batch):\n",
        "    return tf.concat([X_batch, X_batch[:, :5]], axis=1), y_batch\n",
        "\n",
        "dataset_complex = dataset_complex.map(add_extra_features).prefetch(1)\n",
        "\n",
        "for X_batch_sample, y_batch_sample in dataset_complex.take(1):\n",
        "    print(f\"Bentuk X_batch setelah map: {X_batch_sample.shape}\")\n",
        "    print(f\"Bentuk y_batch setelah map: {y_batch_sample.shape}\")\n",
        "\n",
        "# --- Filter item\n",
        "def filter_high_prices(X, y):\n",
        "    return y > 2.0\n",
        "\n",
        "dataset_filter_item = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train))\n",
        "dataset_filter_item = dataset_filter_item.filter(filter_high_prices).batch(batch_size).prefetch(1)\n",
        "\n",
        "count_filtered = 0\n",
        "for _, _ in dataset_filter_item:\n",
        "    count_filtered += 1\n",
        "print(f\"Jumlah batch setelah filter (y > 2.0 per item): {count_filtered}\")\n",
        "\n",
        "# --- Interleave dari file CSV\n",
        "print(\"\\n--- tf.data API: Interleave ---\")\n",
        "\n",
        "def make_dummy_csv_files(num_files=5, num_rows_per_file=100, filename_prefix=\"my_data\"):\n",
        "    dummy_dir = os.path.join(PROJECT_ROOT_DIR, \"dummy_data\")\n",
        "    os.makedirs(dummy_dir, exist_ok=True)\n",
        "    file_paths = []\n",
        "    for i in range(num_files):\n",
        "        filename = f\"{filename_prefix}_{i}.csv\"\n",
        "        filepath = os.path.join(dummy_dir, filename)\n",
        "        df_dummy = pd.DataFrame(np.random.rand(num_rows_per_file, 3), columns=['col1', 'col2', 'col3'])\n",
        "        df_dummy.to_csv(filepath, index=False)\n",
        "        file_paths.append(filepath)\n",
        "    print(f\"Dibuat {num_files} file CSV dummy di {dummy_dir}\")\n",
        "    return file_paths\n",
        "\n",
        "dummy_csv_files = make_dummy_csv_files()\n",
        "\n",
        "def parse_csv(filepath):\n",
        "    return tf.data.TextLineDataset(filepath).skip(1)\n",
        "\n",
        "filepath_dataset = tf.data.Dataset.from_tensor_slices(dummy_csv_files)\n",
        "\n",
        "interleaved_dataset = filepath_dataset.interleave(\n",
        "    parse_csv, cycle_length=2, block_length=1\n",
        ")\n",
        "\n",
        "print(\"\\nInterleaved data (beberapa baris):\")\n",
        "for line in interleaved_dataset.take(5):\n",
        "    print(line.numpy().decode(\"utf-8\"))\n",
        "\n",
        "# --- TFRecord Serialization ---\n",
        "print(\"\\n--- Menguraikan Data TensorFlow Record ---\")\n",
        "\n",
        "def serialize_example(x, y):\n",
        "    feature = {\n",
        "        \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=x.ravel())),\n",
        "        \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=[y]))\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "tfrecord_path = os.path.join(PROJECT_ROOT_DIR, \"my_housing_data.tfrecord\")\n",
        "with tf.io.TFRecordWriter(tfrecord_path) as writer:\n",
        "    for x, y in zip(X_train_scaled, y_train):\n",
        "        writer.write(serialize_example(x, y))\n",
        "print(\"Data berhasil ditulis ke TFRecord.\")\n",
        "\n",
        "# Parsing TFRecord\n",
        "def parse_tfrecord_example(example_proto):\n",
        "    feature_description = {\n",
        "        \"features\": tf.io.FixedLenFeature([X_train_scaled.shape[1]], tf.float32),\n",
        "        \"label\": tf.io.FixedLenFeature([1], tf.float32)\n",
        "    }\n",
        "    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    return parsed_features[\"features\"], parsed_features[\"label\"]\n",
        "\n",
        "tfrecord_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "parsed_tfrecord_dataset = tfrecord_dataset.map(parse_tfrecord_example).batch(3).prefetch(1)\n",
        "\n",
        "print(\"\\nMembaca dan menguraikan TFRecord (beberapa contoh):\")\n",
        "for features_batch, labels_batch in parsed_tfrecord_dataset.take(1):\n",
        "    print(f\"Bentuk fitur: {features_batch.shape}\")\n",
        "    print(f\"Bentuk label: {labels_batch.shape}\")\n",
        "    print(f\"Contoh fitur (pertama): {features_batch[0].numpy()}\")\n",
        "    print(f\"Contoh label (pertama): {labels_batch[0].numpy()}\")\n",
        "\n",
        "# --- Keras Preprocessing Layers ---\n",
        "print(\"\\n--- Keras Preprocessing Layers ---\")\n",
        "\n",
        "model_preprocess_layer = keras.models.Sequential([\n",
        "    keras.layers.Normalization(axis=-1),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "print(\"Mengadaptasi Normalization layer...\")\n",
        "model_preprocess_layer.layers[0].adapt(X_train)  # menggunakan data non-scaled\n",
        "print(\"Normalization layer berhasil diadaptasi.\")\n",
        "\n",
        "model_preprocess_layer.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "print(\"\\nMelatih model dengan Normalization layer...\")\n",
        "history_preprocess = model_preprocess_layer.fit(X_train, y_train, epochs=5, verbose=0)\n",
        "print(\"Model dengan Normalization layer berhasil dilatih.\")\n",
        "print(f\"Loss terakhir: {history_preprocess.history['loss'][-1]:.4f}\")\n",
        "\n",
        "print(\"\\n--- Selesai Reproduksi Kode Chapter 13 ---\")\n"
      ]
    }
  ]
}